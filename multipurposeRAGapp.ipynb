{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "- deploy to huggingface spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/Desktop/Creative/Coding/RagAppUI/RAGAppUI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "from anthropic import Anthropic\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_api_key = None # ask Isaac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/Desktop/Creative/Coding/RagAppUI/RAGAppUI/.venv/lib/python3.12/site-packages/gradio/utils.py:928: UserWarning: Expected at least 3 arguments for function <function encode_docs at 0x1051b6e80>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def encode_docs(uploaded_files:list, encoding_folder:str, mode, progress=gr.Progress()):\n",
    "    encoding_folder = os.getcwd() + f\"/{encoding_folder}\"\n",
    "    print(encoding_folder)\n",
    "    # creating folder and children\n",
    "    Path(encoding_folder).mkdir(parents=True, exist_ok=True) # create dir if not exist\n",
    "    (Path(encoding_folder) / \"txts\").mkdir(parents=True, exist_ok=True) # create txt folder\n",
    "    (Path(encoding_folder) / \"chunks\").mkdir(parents=True, exist_ok=True) # create chunks folder\n",
    "\n",
    "    # gathering all files specified in app\n",
    "    uploaded_files = [] if uploaded_files is None else uploaded_files\n",
    "\n",
    "    # coverting pdf -> txt, copying all files to new encoding folder\n",
    "    for file in progress.tqdm(uploaded_files, desc=\"Loading Files\"):\n",
    "        file_name = file.split('/')[-1]\n",
    "        file_stem = file_name.split('.')[-1]\n",
    "        if file_stem == \"pdf\":\n",
    "            reader = PdfReader(file)\n",
    "            number_of_pages = len(reader.pages)\n",
    "            text = \"\"\n",
    "            for page_num in range(number_of_pages):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "        if file_stem == \"txt\":\n",
    "            text = open(file).read()\n",
    "        file_name = file_name.split(\".\")[0] + \".txt\"\n",
    "        f = open(encoding_folder + f\"/txts/{file_name}\", \"a\")\n",
    "        f.write(text)\n",
    "\n",
    "    # load txt files into langchain\n",
    "    progress(0, desc=\"Loading Langchain Text Splitter\")\n",
    "    loader = DirectoryLoader(Path(encoding_folder)/\"txts\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "\n",
    "    # splitting documents into manageable-sized chunks (thanks langchain!)\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator = \"\\n\",\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap  = 100)\n",
    "    document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    document_chunks=[f\"Context: {chunk.page_content} Source: {chunk.metadata['source']}\" for chunk in progress.tqdm(document_chunks, desc=\"Splitting Documents\")]\n",
    "\n",
    "    # create embeddings for txt chunks\n",
    "    progress(0, desc=\"Loading HuggingFace Encoder (may take several minutes)\")\n",
    "    embeddings = HuggingFaceEmbeddings() # load huggingface text embeddings (transform documents to numbers for later comparison)\n",
    "\n",
    "    progress(0, desc=\"creating pandas dataframe\")\n",
    "    df = pd.DataFrame(document_chunks, columns =['text'])\n",
    "    index_embeddings = []\n",
    "\n",
    "    progress((0, len(df)), desc=\"generating embeddings\")\n",
    "    for i, (index, doc) in enumerate(df.iterrows()):\n",
    "        finished = 0\n",
    "        embedding = embeddings.embed_query(doc[\"text\"])\n",
    "        if embedding is not None:\n",
    "            doc_id=f\"{index}.txt\"\n",
    "            embedding_dict = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "            }\n",
    "            index_embeddings.append(json.dumps(embedding_dict) + \"\\n\")\n",
    "            doc_id = f\"{index}.txt\"\n",
    "            with open(f\"{encoding_folder}/chunks/{doc_id}\", \"w\") as document:\n",
    "                document.write(doc['text'])\n",
    "        with open(Path(encoding_folder)/\"embeddings.json\", \"w\") as f:\n",
    "            f.writelines(index_embeddings)\n",
    "        finished += 1\n",
    "        progress((i, len(df)), desc=\"generating embeddings\")\n",
    "    return True, embeddings\n",
    "\n",
    "def load_encodings(encoding_folder:str, mode,  progress=gr.Progress()):\n",
    "    encoding_folder = os.getcwd() + f\"/{encoding_folder}\"\n",
    "    progress((0, 1), desc=\"reading encodings\")\n",
    "    embeddings_json = encoding_folder+\"/embeddings.json\" # getting embedded data from previous steps\n",
    "    file = open(embeddings_json)\n",
    "    line = file.readline()\n",
    "    full_array = []\n",
    "    while line: # loading embeddings into memory -> numpy array\n",
    "        embed = json.loads(line)['embedding']\n",
    "        full_array.append(embed)\n",
    "        line = file.readline()\n",
    "    embeddings_array = np.array(full_array, dtype=np.float32)\n",
    "\n",
    "    # creating k nearest neighbors object\n",
    "    n_neighbors = 8\n",
    "    progress((0, 1), desc=\"clustering data\")\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nbrs.fit(embeddings_array)\n",
    "\n",
    "    tokenizer, model = None, None\n",
    "    if mode:\n",
    "        progress((0, 2), desc=\"loading text models\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\") # loading tokenizer for flan\n",
    "        progress((1, 2), desc=\"loading text models\")\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\") # loading flan text generation model\n",
    "    else:\n",
    "        model = Anthropic(api_key=claude_api_key)\n",
    "\n",
    "    return True, nbrs, tokenizer, model\n",
    "\n",
    "def generate_response(question:str, encoding_folder, embeddings, nbrs, tokenizer, model, mode, progress=gr.Progress()):\n",
    "    encoding_folder = os.getcwd() + f\"/{encoding_folder}\"\n",
    "    if embeddings is None:\n",
    "        progress((0,1), desc=\"loading embeddings\")\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "    progress((0,1), desc=\"embedding question\")\n",
    "    embedding = embeddings.embed_query(question) # embed question to latent space using huggingface embeddings\n",
    "    progress((0,1), desc=\"getting relevant documents\")\n",
    "    distances, indices = nbrs.kneighbors([embedding]) # get 8 most similar\n",
    "\n",
    "    file = open(encoding_folder+f\"/chunks/{indices[0][0]}.txt\", 'r')\n",
    "    context = file.read()\n",
    "    # for i in range(min([amount_of_context, len(indices[0]), n_neighbors] )): # iterate over each found document\n",
    "    #     file = open(warframe_text_chunks_folder / f\"{indices[0][i]}.txt\", 'r')\n",
    "    #     context += file.read() # open its text\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "    Follow exactly these 3 steps:\n",
    "    1. Read the context below and aggregrate this data\n",
    "    2. Answer the question using only this context\n",
    "    3. Show the source for your answers\n",
    "    If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "    Context : {context}\n",
    "    User Question: {question}\n",
    "    \"\"\"\n",
    "    progress((0,1), desc=\"prompting model\")\n",
    "\n",
    "    if mode:\n",
    "        model_input = tokenizer(prompt, return_tensors=\"pt\").input_ids # tokenizing prompt for model\n",
    "        model_output = model.generate(model_input, min_length=100, max_length=2000) # generating response\n",
    "        text_output = tokenizer.decode(model_output[0]) # decoding response\n",
    "    else:\n",
    "\n",
    "        message = model.messages.create(\n",
    "        max_tokens=512,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        # model=\"claude-3-opus-20240229\",\n",
    "        # model = \"claude-3-sonnet-20240229\"\n",
    "        model = \"claude-3-haiku-20240307\",\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    return text_output\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # \"global\" variables\n",
    "    embeddings = gr.State(None)\n",
    "    nbrs = gr.State(None)\n",
    "    tokenizer = gr.State(None)\n",
    "    model = gr.State(None)\n",
    "    with gr.Column() as outer_col:\n",
    "        with gr.Accordion(label=\"Instructions\", open=False) as instructions:\n",
    "            instructions = open(os.getcwd()+\"/instructions.md\").read()\n",
    "            gr.Markdown(value=instructions)\n",
    "        with gr.Row() as row:\n",
    "            with gr.Column() as col1:\n",
    "                mode = gr.Checkbox(False, label=\" run model locally (Google Flan) \\n (if not checked, uses Anthropic Claude API)\")\n",
    "                files = gr.Files(label=\"pdfs or txt files\", file_types=[\".pdf\", \".txt\"])\n",
    "                encodings_folder = gr.Textbox(label=\"Encodings Name\", value=\"my_awesome_encodings\")\n",
    "                encode_button = gr.Button(\"Create Document Encodings\")\n",
    "                encode_check = gr.Checkbox(value=False, interactive=False, label=\"\")\n",
    "                load_button = gr.Button(\"Load Document Encodings\", scale=8)\n",
    "                load_check = gr.Checkbox(value=False, interactive=False, label=\"\", scale=1, min_width=0)\n",
    "                encode_button.click(fn=encode_docs, inputs=[files, encodings_folder], outputs=[encode_check, embeddings])\n",
    "                load_button.click(fn=load_encodings, inputs=[encodings_folder, mode], outputs=[load_check, nbrs, tokenizer, model])\n",
    "            with gr.Column() as col2:\n",
    "                user_input = gr.Textbox(label=\"Question\")\n",
    "                generate_button = gr.Button(\"Generate Response\")\n",
    "                textbox = gr.Text(value=\"...\", label=\"Response\")\n",
    "                generate_button.click(fn=generate_response, inputs=[user_input, encodings_folder, embeddings, nbrs, tokenizer, model, mode], outputs=[textbox])\n",
    "demo.launch()#share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
